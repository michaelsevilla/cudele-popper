%\section{Discussion}
%
%% Could we have implemented this on something other than CephFS?
%One question that comes up often in our work with Ceph is: ``Why do we always
%choose Ceph and can we implement this on another storage system?" Ceph is
%popular, robust, and open source -- getting it merged into mainline has a
%better chance of getting used than if we had built the system `clean-slate'
%from the ground up. Using Ceph, we can also leverage its robustness using an
%approach called ``programmability".
%
%% How awesome programmability is
%``Programmability" is a way of designing storage systems that encourages code
%re-use and composition.  A programmable storage system exposes internal
%subsystem as building blocks for higher level services. This `dirty-slate'
%approach limits redundant code and leverages the robustness of the underlying
%storage system. Cudele uses this approach and re-uses some of the building
%blocks from the Malacology programmable storage system~\cite{sevilla:eurosys17}
%to great success and requires only:
%
%\begin{itemize}
%
%  \item 354 lines of library code
%
%  \item 219 lines of non-destructive metadata server code, which is not used
%  unless it is turned on
%
%  \item 4 lines of destructive client/server code to check whether a namespace
%  is decoupled
%
%\end{itemize}

\section{Conclusion and Future Work}

Relaxing consistency/durability semantics in file systems is a double-edged
sword. While the technique performs and scales better, it alienates applications that rely
on strong consistency and durability.  Cudele lets
\oldcomment{users}\newcomment{administrators} assign consistency/durability
guarantees to subtrees in the global namespace, resulting in 
custom fit semantics for applications. We show how applications can co-exist and
perform well in a global namespace and our prototype enables studies that
adjust these semantics over {\it time and space}, where subtrees can change
without ever moving the data they reference.

%This approach is superior to mounting different storage systems in a global
%namespace because there is no need to provision dedicated storage clusters to
%applications or move data between these systems.  
Cudele prompts many avenues for future work.  First is to co-locate HPC
workflows with real highly parallel runtimes from the cloud in the same
namespace. This setup would show how Cudele reasonably incorporates both programming
models (client-driven parallelism and user-defined workflows) at the same time
and should show large performance gains.  Second is dynamically changing
semantics of a subtree from stronger to weaker guarantees (or vice versa). This
reduces data movement across storage cluster and file system boundaries so
the results of a Hadoop job do not need to be migrated into CephFS for
other processing; instead the \oldcomment{user}\newcomment{administrator} can
change the semantics of the HDFS subtree into a CephFS subtree, which may cause
metadata/data movement to ensure strong consistency. Third is embeddable
policies, where child subtrees have specialized features but still maintain
guarantees of their parent subtrees.  For example, a RAMDisk subtree is POSIX
IO-compliant but relaxes durability constraints, so it can reside under a POSIX
IO subtree alongside a globally durable subtree.

%Finally, our prototype enables
%performance prediction because it helps administrators quantify the costs of
%each mechanism (similar to Section~\S\ref{sec:microbenchmarks}).

%Applications can use Cudele to microbenchmark their components and software,
%similar to what we did in 

%Using those
%results, they can predict how much slower their system will be if they adopt
%stronger consistency or durability.  This is a form of co-design that takes a
%``dirty-slate" approach but building just the guarantees the application needs
%from existing implementations.  
%This can also be a verification tool where performance that varies wildly from
%the predicted performance can be a red flag that something is wrong or that
%the bottleneck is not in the consistency or durability plane.



% Implied Namespaces

% Intermediate Update Bursts

% See if this helps load balancing

% executing mechanisms in parallel

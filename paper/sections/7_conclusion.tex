%\section{Discussion}
%
%% Could we have implemented this on something other than CephFS?
%One question that comes up often in our work with Ceph is: ``Why do we always
%choose Ceph and can we implement this on another storage system?" Ceph is
%popular, robust, and open source -- getting it merged into mainline has a
%better chance of getting used than if we had built the system `clean-slate'
%from the ground up. Using Ceph, we can also leverage its robustness using an
%approach called ``programmability".
%
%% How awesome programmability is
%``Programmability" is a way of designing storage systems that encourages code
%re-use and composition.  A programmable storage system exposes internal
%subsystem as building blocks for higher level services. This `dirty-slate'
%approach limits redundant code and leverages the robustness of the underlying
%storage system. Cudele uses this approach and re-uses some of the building
%blocks from the Malacology programmable storage system~\cite{sevilla:eurosys17}
%to great success and requires only:
%
%\begin{itemize}
%
%  \item 354 lines of library code
%
%  \item 219 lines of non-destructive metadata server code, which is not used
%  unless it is turned on
%
%  \item 4 lines of destructive client/server code to check whether a namespace
%  is decoupled
%
%\end{itemize}

\section{Conclusion and Future Work}

Relaxing consistency/durability semantics in file systems is a double-edged
sword. While the technique performs and scales better, it alienates applications that rely
on strong consistency and durability.  Cudele lets
\oldcomment{users}\newcomment{administrators} assign consistency/durability
guarantees to subtrees in the global namespace, resulting in 
custom fit semantics for applications. We show how applications can co-exist and
perform well in a global namespace and our prototype enables studies that
adjust these semantics over {\it time and space}, where subtrees can change
without moving data they reference.`

%This approach is superior to mounting different storage systems in a global
%namespace because there is no need to provision dedicated storage clusters to
%applications or move data between these systems.  
\textbf{Future Work}: First is to co-locate HPC workflows with real highly
parallel runtimes from the cloud in the same namespace. This setup would show
how Cudele reasonably incorporates both programming models at the same time.
Second is dynamically changing semantics of a subtree from stronger to weaker
guarantees. This reduces data movement across storage cluster and file system
boundaries so the results of a Hadoop job do not need to be migrated into
CephFS for other processing; instead the
\oldcomment{user}\newcomment{administrator} can change the semantics of the
HDFS subtree into a CephFS subtree. Third is embeddable policies, where child
subtrees have specialized features but still maintain guarantees of the
parent subtree.  For example, a RAMDisk subtree is POSIX IO-compliant but
relaxes durability, so it can reside under a POSIX IO subtree
alongside a globally durable subtree.\vspace{-1ex}

%Finally, our prototype enables
%performance prediction because it helps administrators quantify the costs of
%each mechanism (similar to Section~\S\ref{sec:microbenchmarks}).

%Applications can use Cudele to microbenchmark their components and software,
%similar to what we did in 

%Using those
%results, they can predict how much slower their system will be if they adopt
%stronger consistency or durability.  This is a form of co-design that takes a
%``dirty-slate" approach but building just the guarantees the application needs
%from existing implementations.  
%This can also be a verification tool where performance that varies wildly from
%the predicted performance can be a red flag that something is wrong or that
%the bottleneck is not in the consistency or durability plane.



% Implied Namespaces

% Intermediate Update Bursts

% See if this helps load balancing

% executing mechanisms in parallel

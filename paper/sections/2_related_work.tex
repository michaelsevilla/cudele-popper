\section{Related Work} 
\label{sec:related-work}

%% General
%The bottlenecks associated with accessing POSIX IO file system metadata are not
%limited to HPC workloads and the same challenges that plagued these systems for
%years are finding their way into the cloud. Workloads that deal with many small
%files ({\it e.g.}, log processing and database
%queries~\cite{thusoo:sigmod2010-facebook-infrastructure}) and large numbers of
%simultaneous clients ({\it e.g.}, MapReduce
%jobs~\cite{mckusick:acm2010-gfs-evolution}), are subject to the scalability of
%the metadata service. The biggest challenge is that whenever a file is touched
%the client must access the file's metadata and maintaining a file system
%namespace imposes small, frequent accesses on the underlying storage
%system~\cite{roselli:atec2000-FS-workloads}.  Unfortunately, scaling file
%system metadata is a well-known problem and solutions for scaling data IO do
%not work for metadata IO~\cite{roselli:atec2000-FS-workloads,
%abad:ucc2012-mimesis, alam:pdsw2011-metadata-scaling, weil:osdi2006-ceph}. 
%There are two approaches for improving the performance of metadata access.
%
%\subsection{Metadata Load Balancing}
%
%% approaches to load balancing (strong = more resuorces per unit work, weak =
%% fixed resource per work unit) % CITEME One approach for improving metadata
%performance and scalability is to alleviate overloaded servers by load
%balancing metadata IO across a cluster. Common techniques include partitioning
%metadata when there are many writes and replicating metadata when there are
%many reads. For example, IndexFS~\cite{ren:sc2014-indexfs} partitions
%directories and clients write to different partitions by grabbing leases and
%caching ancestor metadata for path traversal; it does well for strong scaling
%because servers can keep more inodes in the cache which results in less RPCs.
%Alternatively, ShardFS replicates directory state so servers do not need to
%contact peers for path traversal; it does well for read workloads because all
%file operations only require 1 RPC and for weak scaling because requests will
%never incur extra RPCs due to a full cache.  CephFS employs both techniques to
%a lesser extent; directories can be replicated or sharded but the caching and
%replication policies do not change depending on the balancing technique.
%Despite the performance benefits these techniques add complexity and
%jeopardize the robustness and performance characteristics of the metadata
%service because the systems now need (1) policies to guide the migration
%decisions and (2) mechanisms to address inconsistent states across servers.
%
%% policies Setting policies for migrations is arguably more difficult than
%adding the migration mechanisms themselves.  For example, IndexFS and CephFS
%use the GIGA+~\cite{patil:fast2011-giga} technique for partitioning
%directories at a predefined threshold and using lazy synchronization to
%redirect queries to the server that ``owns" the targeted metadata.
%Determining when to partition directories and when to migrate the directory
%fragments are policies that vary between systems: GIGA+ partitions directories
%when the size reaches a certain number of files and migrates directory
%fragments immediately; CephFS partitions directories when they reach a
%threshold size or when the write temperature reaches a certain value and
%migrates directory fragments when the hosting server has more load than the
%other servers in the metadata cluster. Another policy is when and how to
%replicate directory state; ShardFS replicates immediately and pessimistically
%while CephFS replicates only when the read temperature reaches a threshold.
%There is a wide range of policies and it is difficult to maneuver tunables and
%hard-coded design decisions.
%
%% addressing inconsistency In addition to the policies, distributing metadata
%across a cluster requires distributed transactions and cache coherence
%protocols to ensure strong consistency ({e.g.}, POSIX IO).  For example,
%ShardFS pessimistically replicates directory state and uses optimistic
%concurrency control for conflicts; namely it does the operation and if there
%is a conflict at verification time it falls back to two-phase locking.
%Another example is IndexFS's inode cache which reduces RPCs by caching
%ancestor paths -- the locality of this cache can be thrashed by random reads
%but performs well for metadata writes. For consistency, writes to directories
%in IndexFS block until the lease expires while writes to directories in
%ShardFS are slow for everyone as it either requires serialization or locking
%with many servers; reads in IndexFS are subject to cache locality while reads
%in ShardFS always resolve to 1 RPC.  Another example of the overheads of
%addressing inconsistency is how CephFS maintains client sessions and inode
%caches for capabilities (which in turn make metadata access faster). When
%metadata is exchanged between metadata servers these sessions/caches must be
%flushed and new statistics exchanged with a scatter-gather process; this halts
%updates on the directories and blocks until the authoritative metadata server
%responds.  These protocols are discussed in more detail in the next section
%but their inclusion here is a testament to the complexity of migrating
%metadata.
%
%%For metadata writes it does a distributed transaction; monotonic writes with
%%concurrent clients fail and do pessimistic locking through a mediated lock
%%server to ensure strong consistency, non-monotonic writes grab locks at every
%%server. Zooming in on monotonic writes: if permissions increase it executes
%on %a primary then non-primary, if permissions decrease it executes on all
%%non-primary then on primary.
%
%The conclusion we have drawn from this related work is that metadata protocols
%have a bigger impact on performance and scalability than load balancing.
%Understanding these protocols helps load balancing and gives us a better
%understanding of the metrics we should use to make migration decisions ({\it
%e.g.}, which operations reflect the state of the system), what types of
%requests cause the most load, and how an overloaded system reacts ({\it e.g.},
%increasing latencies, lower throughput, etc.).

%\subsection{Relaxing POSIX IO}

% POSIX IO 
POSIX IO workloads require strong consistency and many file systems improve
performance by reducing the number of remote calls per operation ({\it i.e.}
RPC amplification). As discussed in the previous section, caching with leases and
replication are popular approaches to reducing the overheads of path traversals
but their performance is subject to cache locality and the amount of available
resources, respectively; for random workloads larger than the cache extra RPCs
hurt performance~\cite{ren:sc2014-indexfs, weil:sc2004-dyn-metadata} and for write heavy workloads with more
resources the RPCs for invalidations are harmful. Another approach to reducing
RPCs is to use leases or capabilities.  


%IndexFS aggressively caches paths and
%handles permissions by handing out leases for metadata writes; metadata may
%only be modified when all leases have expired. 
%IndexFS~\cite{ren:sc2014-indexfs} aggressively caches pathnames and their
%permissions on the client servers. Modifications to metadata cached by clients
%is delayed until all client leases have expired. This reduces the RPC
%amplification to 1 when mutatating directory metadata ({\it e.g.,}
%\texttt{mkdir}, \texttt{chmod}, etc.) because clients are not querying metadata servers for
%path traversals. The disadvantage of this approach is the high latency of
%mutation operations (reads to filenames).  ShardFS~\cite{xiao:socc2015-shardfs}
%replicates metadata (specifically directory lookup state) across the metadata server
%cluster, reducing the RPC amplification to 1 for file operations ({e.g.,}
%\texttt{stat}, \texttt{chmod}, \texttt{chown}, etc.). Modifications to the
%directory lookup state are done with optimistic concurrecny control and fall
%back to retry if verification fails. The disadvantage of this appraoch is the
%high number of RPCs for maintaining directory metadata mutations (writes to
%directories).  CephFS also maintains an inode cache but its notion of leases
%are much shorter, on the order of microseconds.

% Non POSIX IO
High performance computing has unique requirements for file systems and
well-defined workloads that make relaxing POSIX IO sensible.
BatchFS~\cite{zheng:pdsw2014-batchfs} assumes the application coordinates
accesses to the namespace, so the clients can batch local operations and merge
with a global namespace image lazily. Similarly,
DeltaFS~\cite{zheng:pdsw2015-deltafs} eliminates RPC traffic using subtree
snapshots for non-conflicting workloads and middleware for conflicting
workloads. MarFS~\cite{grider:pdsw2015-marfs} gives users the ability to lock
``project directories" and allocate clusters to demanding metadata
workloads.  Unfortunately, decoupling the namespace has costs: (1) merging
metadata state back into the global namespace is slow; (2) failures are local
to the failing node; and (3) the systems are not backwards compatible.

% NON POSIX IO
%Decoupling the namespaces has many advantages, including improved scalability,
%higher resource utilization, and better performance.  BatchFS and DeltaFS are
%near-POSIX IO filesystems that give clients the ability to decouple subtrees from
%the namespace so that the applications can execute metadata operations without
%synchronization and serialization.  These operations are applied to a local
%snapshot of the file system namespace and conflicts are resolved either by the
%application or by an external service . Applications link into a metadata
%server library to reduce resource utilization and code paths (e.g., no daemons
%and less interprocess communication). 

For (1), state-of-the-art systems manage consistency in non-traditional ways:
IndexFS~\cite{ren:sc2014-indexfs} maintains the global namespace but blocks operations from other clients
until the first client drops the lease, BatchFS does operations on a snapshot
of the namespace and merges batches of operations into the global namespace,
and DeltaFS never merges back into the global namespace. The merging for
BatchFS is done by an auxiliary metadata server running on the client and
conflicts are resolved by the application. Although DeltaFS never explicitly
merges, applications needing some degree of ground truth can either manage
consistency themselves on a read or add a bolt-on service to manage the
consistency. For (2), if the client fails and stays down, all metadata
operations on the decoupled namespace are lost. If the client recovers, the
on-disk structures (for BatchFS and DeltaFS this is the SSTables used in
TableFS~\cite{ren:atc2013-tablefs}) can be recovered. In other words, the clients have state that cannot
be recovered if the node stays failed and any progress will be lost. This
scenario is a disaster for checkpoint-restart where missed cycles may cause the
checkpoint to bleed over into computation time. For (3), decoupled namespace
approaches sacrifice POSIX IO going as far as requiring the application to link
against the systems they want to talk to. In today's world of software defined
caching, this can be a problem for large data centers with many types and tiers
of storage. Despite well-known performance problems POSIX IO and REST are the
dominant APIs for data transfer.
